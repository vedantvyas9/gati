"""
Integration test with REAL OpenAI API calls to verify the entire SDK works end-to-end.

This test:
1. Loads OpenAI API key from .env
2. Makes actual LLM calls
3. Verifies GATI tracks all events correctly
4. Tests token counting and cost calculation with real responses
5. Validates event serialization and buffering
"""

import os
import json
import time
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env (project root)
env_path = Path(__file__).parent.parent / ".env"
load_dotenv(env_path)

# Verify API key is loaded
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY not found in .env file")

print(f"‚úì Loaded OpenAI API key (first 20 chars): {OPENAI_API_KEY[:20]}...")

# Import GATI SDK
from gati import observe
from gati.core.event import LLMCallEvent, ToolCallEvent
from gati.core.context import run_context, get_current_run_id

# Import LangChain
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool


# ============================================================================
# TEST 1: Basic LLM Call with GATI Tracking
# ============================================================================

def test_basic_llm_call():
    """Test 1: Basic ChatGPT call with GATI tracking"""
    print("\n" + "="*70)
    print("TEST 1: Basic LLM Call with GATI Tracking")
    print("="*70)

    # Initialize GATI SDK (no backend needed for this test)
    # Events will be buffered but we'll inspect them directly
    observe.init(
        backend_url="http://localhost:9999",  # Non-existent, but OK for this test
        agent_name="test_agent",
        auto_inject=False  # We'll use explicit callbacks
    )

    # Create LLM with explicit GATI callbacks
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        callbacks=observe.get_callbacks(),
        temperature=0.7
    )

    print("\nüìù Sending prompt to OpenAI...")
    response = llm.invoke("What is 2 + 2? Answer in one word.")

    print(f"‚úì Got response: {response.content}")

    # Give time for events to be buffered
    time.sleep(0.5)

    # Check if events were captured
    if observe._buffer:
        buffered_events = observe._buffer._events
        print(f"\nüìä Events in buffer: {len(buffered_events)}")
        for i, event in enumerate(buffered_events, 1):
            print(f"\n  Event {i}:")
            print(f"    Type: {event.event_type}")
            print(f"    Run ID: {event.run_id[:8]}..." if event.run_id else "    Run ID: (none)")
            print(f"    Agent: {event.agent_name}")
            if hasattr(event, 'model'):
                print(f"    Model: {event.model}")
            if hasattr(event, 'tokens_in'):
                print(f"    Tokens In: {event.tokens_in}")
            if hasattr(event, 'tokens_out'):
                print(f"    Tokens Out: {event.tokens_out}")
            if hasattr(event, 'cost'):
                print(f"    Cost: ${event.cost:.6f}")
            if hasattr(event, 'latency_ms'):
                print(f"    Latency: {event.latency_ms:.2f}ms")

    observe.shutdown()
    print("\n‚úì Test 1 passed!")


# ============================================================================
# TEST 2: LLM with Tool Calls
# ============================================================================

def test_llm_with_tools():
    """Test 2: Agent with tool calls"""
    print("\n" + "="*70)
    print("TEST 2: LLM with Tool Calls")
    print("="*70)

    # Define tools
    @tool
    def add(a: int, b: int) -> int:
        """Add two numbers"""
        return a + b

    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply two numbers"""
        return a * b

    tools = [add, multiply]

    # Initialize GATI
    observe.init(
        backend_url="http://localhost:9999",
        agent_name="math_agent",
        auto_inject=False
    )

    # Create LLM with callbacks
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        callbacks=observe.get_callbacks(),
    )

    # Create agent
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful math assistant. Use the provided tools to solve math problems."),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    agent = create_tool_calling_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        callbacks=observe.get_callbacks(),
        verbose=False
    )

    print("\nü§ñ Running agent with tools...")
    result = agent_executor.invoke({
        "input": "What is 25 times 4? Then add 10 to the result."
    })

    print(f"\n‚úì Agent result: {result['output']}")

    # Give time for events
    time.sleep(0.5)

    # Count event types
    if observe._buffer:
        events = observe._buffer._events
        event_types = {}
        for event in events:
            event_types[event.event_type] = event_types.get(event.event_type, 0) + 1

        print(f"\nüìä Event Summary:")
        for event_type, count in event_types.items():
            print(f"  {event_type}: {count}")

        # Show token usage
        total_tokens_in = 0
        total_tokens_out = 0
        total_cost = 0.0

        for event in events:
            if hasattr(event, 'tokens_in'):
                total_tokens_in += event.tokens_in
            if hasattr(event, 'tokens_out'):
                total_tokens_out += event.tokens_out
            if hasattr(event, 'cost'):
                total_cost += event.cost

        print(f"\nüí∞ Total Usage:")
        print(f"  Input Tokens: {total_tokens_in}")
        print(f"  Output Tokens: {total_tokens_out}")
        print(f"  Total Cost: ${total_cost:.6f}")

    observe.shutdown()
    print("\n‚úì Test 2 passed!")


# ============================================================================
# TEST 3: Context Tracking with Nested Runs
# ============================================================================

def test_context_tracking():
    """Test 3: Context tracking and nested runs"""
    print("\n" + "="*70)
    print("TEST 3: Context Tracking with Nested Runs")
    print("="*70)

    observe.init(
        backend_url="http://localhost:9999",
        agent_name="context_test",
        auto_inject=False
    )

    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        callbacks=observe.get_callbacks(),
    )

    print("\nüîó Testing nested execution contexts...")

    # Parent context
    with run_context() as parent_id:
        print(f"Parent run ID: {parent_id[:8]}...")

        # First LLM call in parent context
        response1 = llm.invoke("What is the capital of France?")
        print(f"‚úì Parent call 1: {response1.content}")

        # Child context
        with run_context() as child_id:
            print(f"  Child run ID: {child_id[:8]}...")

            # LLM call in child context
            response2 = llm.invoke("What is the capital of Germany?")
            print(f"  ‚úì Child call: {response2.content}")

        # Back to parent context
        response3 = llm.invoke("What is the capital of Spain?")
        print(f"‚úì Parent call 2: {response3.content}")

    time.sleep(0.5)

    # Verify context tracking
    if observe._buffer:
        events = observe._buffer._events
        print(f"\nüìä Events captured: {len(events)}")

        # Group by run_id
        runs = {}
        for event in events:
            run_id = event.run_id
            if run_id not in runs:
                runs[run_id] = []
            runs[run_id].append(event)

        print(f"üìç Unique run IDs: {len(runs)}")
        for run_id in sorted(runs.keys()):
            print(f"  {run_id[:8]}...: {len(runs[run_id])} events")

    observe.shutdown()
    print("\n‚úì Test 3 passed!")


# ============================================================================
# TEST 4: Event Serialization
# ============================================================================

def test_event_serialization():
    """Test 4: Verify events are JSON serializable"""
    print("\n" + "="*70)
    print("TEST 4: Event Serialization")
    print("="*70)

    observe.init(
        backend_url="http://localhost:9999",
        agent_name="serialization_test",
        auto_inject=False
    )

    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        callbacks=observe.get_callbacks(),
    )

    print("\n‚úçÔ∏è Making LLM call for serialization test...")
    response = llm.invoke("Say 'Hello' in one word.")
    print(f"‚úì Got response: {response.content}")

    time.sleep(0.5)

    # Serialize all events
    if observe._buffer:
        events = observe._buffer._events
        print(f"\nüîÑ Serializing {len(events)} events...")

        for i, event in enumerate(events, 1):
            try:
                # Convert to dict
                event_dict = event.to_dict()

                # Try to serialize to JSON
                json_str = json.dumps(event_dict, indent=2, default=str)

                print(f"\n  Event {i} (JSON length: {len(json_str)} chars):")
                print(f"    ‚úì Successfully serialized")

                # Show key fields
                if 'model' in event_dict:
                    print(f"    Model: {event_dict['model']}")
                if 'event_type' in event_dict:
                    print(f"    Type: {event_dict['event_type']}")
                if 'tokens_in' in event_dict:
                    print(f"    Tokens in: {event_dict['tokens_in']}")

            except Exception as e:
                print(f"  ‚úó Event {i} serialization failed: {e}")

    observe.shutdown()
    print("\n‚úì Test 4 passed!")


# ============================================================================
# TEST 5: Token Counting Accuracy
# ============================================================================

def test_token_counting():
    """Test 5: Verify token counting is accurate"""
    print("\n" + "="*70)
    print("TEST 5: Token Counting Accuracy")
    print("="*70)

    from gati.utils.token_counter import count_tokens, extract_tokens_from_response

    # Test basic token counting
    test_text = "What is the capital of France?"
    token_count = count_tokens(test_text, model="gpt-3.5-turbo")
    print(f"\nüìù Test text: '{test_text}'")
    print(f"   Token count: {token_count}")

    # Make actual LLM call
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
    )

    print(f"\nü§ñ Making actual LLM call...")
    response = llm.invoke(test_text)

    # Extract tokens from response
    tokens = extract_tokens_from_response(response)
    print(f"\n‚úì Extracted tokens from response:")
    print(f"  Prompt tokens: {tokens.get('prompt_tokens', 0)}")
    print(f"  Completion tokens: {tokens.get('completion_tokens', 0)}")
    print(f"  Total tokens: {tokens.get('total_tokens', 0)}")
    print(f"  Response: {response.content}")

    print("\n‚úì Test 5 passed!")


# ============================================================================
# TEST 6: Cost Calculation
# ============================================================================

def test_cost_calculation():
    """Test 6: Verify cost calculation is accurate"""
    print("\n" + "="*70)
    print("TEST 6: Cost Calculation")
    print("="*70)

    from gati.utils.cost_calculator import calculate_cost, normalize_model_name

    test_cases = [
        ("gpt-3.5-turbo", 100, 50),
        ("gpt-4", 100, 100),
        ("claude-3-opus", 100, 100),
        ("gpt-3.5-turbo-0125", 1000, 500),  # Version suffix should be handled
    ]

    print("\nüí∞ Testing cost calculation:")
    for model, input_tokens, output_tokens in test_cases:
        normalized = normalize_model_name(model)
        cost = calculate_cost(model, input_tokens, output_tokens)
        print(f"\n  Model: {model}")
        print(f"    Normalized: {normalized}")
        print(f"    Input: {input_tokens} tokens, Output: {output_tokens} tokens")
        print(f"    Calculated cost: ${cost:.6f}")

    print("\n‚úì Test 6 passed!")


# ============================================================================
# MAIN TEST RUNNER
# ============================================================================

def main():
    """Run all integration tests"""
    print("\n" + "="*70)
    print("GATI SDK INTEGRATION TESTS WITH REAL OPENAI API")
    print("="*70)

    try:
        test_basic_llm_call()
        test_llm_with_tools()
        test_context_tracking()
        test_event_serialization()
        test_token_counting()
        test_cost_calculation()

        print("\n" + "="*70)
        print("‚úì ALL TESTS PASSED!")
        print("="*70)
        print("\nSummary:")
        print("‚úì Real OpenAI API integration working")
        print("‚úì Event tracking and buffering working")
        print("‚úì Token counting accurate")
        print("‚úì Cost calculation working")
        print("‚úì Context tracking functional")
        print("‚úì Event serialization successful")

    except Exception as e:
        print(f"\n‚úó Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
